{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'my': 3, 'dog': 3, 'has': 1, 'flea': 1, 'problems': 1, 'help': 1, 'please': 1, 'maybe': 1, 'not': 1, 'take': 1, 'him': 3, 'to': 2, 'park': 1, 'stupid': 3, 'dalmation': 1, 'is': 1, 'so': 1, 'cute': 1, 'I': 1, 'love': 1, 'stop': 2, 'posting': 1, 'worthless': 2, 'garbage': 1, 'mr': 1, 'licks': 1, 'ate': 1, 'steak': 1, 'how': 1, 'quit': 1, 'buying': 1, 'food': 1})\n",
      "[('to', 0.0322394037469742), ('stop', 0.0322394037469742), ('worthless', 0.0322394037469742), ('my', 0.028288263356383563), ('dog', 0.028288263356383563), ('him', 0.028288263356383563), ('stupid', 0.028288263356383563), ('has', 0.025549122992281622), ('flea', 0.025549122992281622), ('problems', 0.025549122992281622), ('help', 0.025549122992281622), ('please', 0.025549122992281622), ('maybe', 0.025549122992281622), ('not', 0.025549122992281622), ('take', 0.025549122992281622), ('park', 0.025549122992281622), ('dalmation', 0.025549122992281622), ('is', 0.025549122992281622), ('so', 0.025549122992281622), ('cute', 0.025549122992281622), ('I', 0.025549122992281622), ('love', 0.025549122992281622), ('posting', 0.025549122992281622), ('garbage', 0.025549122992281622), ('mr', 0.025549122992281622), ('licks', 0.025549122992281622), ('ate', 0.025549122992281622), ('steak', 0.025549122992281622), ('how', 0.025549122992281622), ('quit', 0.025549122992281622), ('buying', 0.025549122992281622), ('food', 0.025549122992281622)]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import operator\n",
    "\"\"\"\n",
    "函数说明：创建数据样本\n",
    "Returns:\n",
    "    dataset -实验样本切分的词条\n",
    "    classVec -类别标签向量\n",
    "\"\"\"\n",
    "def loadDataSet():\n",
    "    \n",
    "    dataset = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    # 切分的词条\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
    "    classVec=[0,1,0,1,0,1] # 类别标签向量，1代表好，0代表不好\n",
    "    return dataset,classVec\n",
    "\"\"\"\n",
    "函数说明：特征选择TF-IDF算法\n",
    "Parameters:\n",
    "    list_words:词liebiao \n",
    "Returns:\n",
    "    dicr_feature_select:特征选择词字典\n",
    "\"\"\"\n",
    "def feature_select(list_words):\n",
    "    # 总词频统计\n",
    "    doc_frequency=defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i]+=1\n",
    "     \n",
    "    # 计算每个词的TF值\n",
    "    word_tf={ } # 存储每个词的TF值\n",
    "    \n",
    "    for i in doc_frequency:\n",
    "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
    "        \n",
    "    # 计算每个词的IDF值\n",
    "    doc_num=len(list_words)\n",
    "    word_idf={ } # 存储每个词的IDF值\n",
    "    word_doc=defaultdict(int) # 存储包含该词的文档数\n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i]+=1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
    "    \n",
    "    # 计算每个词的TF*IDF的值\n",
    "    word_tf_idf={}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
    "        \n",
    "    # 对字典按值由大到小排序\n",
    "    \n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select\n",
    "if __name__=='__main__':\n",
    "    data_list,label_list=loadDataSet() # 加载数据\n",
    "    features=feature_select(data_list) # 所有词的TF-IDF值\n",
    "    print(features)\n",
    "    print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.688 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['批准', '服务', '后方', '软件开发', '依法', '开展', '活动', '相关']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "text='计算机网络工程及软件产品开发，计算机技术咨询服务；批发、零售：网络设备及配件，通讯设备（不含无线电发射及卫星地面接收设施）。（依法须经批准的项目，经相关部门批准后方可开展经营活动）。 批发：工艺品、皮具箱包、日用品、服装鞋帽、办公用品；软件开发。（依法须经批准的项目，经相关部门批准后方可开展经营活动）。家政服务；家具护理；洗衣服务；三岁以下婴幼儿照护服务；养老服务；人才中介；职业中介；教育咨询服务；软件开发。（依法须经批准的项目，经相关部门批准后方可开展经营活动）劳务派遣服务；人力资源服务（不含职业中介活动）；社会经济咨询服务；技术服务、技术开发、技术咨询、技术交流、技术转让、技术推广；软件开发；个人商务服务；组织文化艺术交流活动。（依法须经批准的项目，经相关部门批准后方可开展经营活动）信息技术咨询服务；信息系统集成服务；软件开发及相关技术服务、技术推广；信息系统设计服务；网络化文化创意设计。（依法须经批准的项目，经相关部门批准后方可开展经营活动）互联网技术服务；网络工程施工；网页制作；企业管理服务；计算机系统服务；软件开发。（依法须经批准的项目，经相关部门批准后方可开展经营活动）销售：电子产品、数码产品、家用电器、通讯设备、转账POS机。计算机软件技术开发、技术转让、技术服务。（依法须经批准的项目，经相关部门批准后方可开展经营活动）医疗器械技术咨询及售后服务；医疗设备安装服务；软件开发；会务服务；商务服务。（依法须经批准的项目，经相关部门批准后方可开展经营活动）'\n",
    "keywords=jieba.analyse.extract_tags(text,topK=8,withWeight=False,allowPOS=())\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA=open(r\"D:\\论文\\cut+TF\\0000172017.csv\",encoding=\"utf-8\")\n",
    "docB=open(r\"D:\\论文\\cut+TF\\0005092017.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "csvreader = csv.reader(docA)\n",
    "final_list = list(csvreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-06e07f9dd975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbowA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbowB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'sat', 'on', 'my', 'bed']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = set(bowA).union(set(bowB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The', 'bed', 'cat', 'dog', 'face', 'my', 'on', 'sat'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
